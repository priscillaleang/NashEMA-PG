"""
Compute exploitability metrics for trained RL agents and inject results into JSON log files.

Usage:
    uv run eval/compute_exploitability.py <log_file_path> <solver_method>
    
    solver_method options:
    - rl: Train exploiting agent via RL (accurate but slow, for Nash PG)
    - from_return: Use logged returns (fast approximation, for FSP/PSRO)

Examples:
    uv run eval/compute_exploitability.py logs/kuhn_poker/nash_pg/mag0.1.json rl
    uv run eval/compute_exploitability.py logs/kuhn_poker/fsp/run1.json from_return
"""

import os
import warnings
import logging
import argparse
os.environ["XLA_PYTHON_CLIENT_PREALLOCATE"] = "false"
# Suppress verbose Orbax checkpoint logging and warnings
logging.getLogger('absl').setLevel(logging.WARNING)
logging.getLogger('orbax').setLevel(logging.ERROR)
# Suppress Orbax sharding warnings
warnings.filterwarnings('ignore', message='.*Sharding info not provided when restoring.*')
from enum import Enum
from functools import partial
from typing import Any, Dict, List, Tuple
from tqdm import tqdm
import jax
from flax import nnx
import chex
import optax
from omegaconf import DictConfig

from envs import create_env
import envs.mytypes as env_types
from agents import create_agent, BaseAgent
from train.core import collect_and_process_trajectories, update_agent

class ExploitabilitySolver(Enum):
    """Enumeration of available exploitability computation methods."""
    RL = "rl"  # Use RL to approximate exploitability (time consuming but accurate)
    FROM_RETURN = "from_return"  # Use logged average return (suitable for FSP/PSRO)



@chex.dataclass
class LearnerState:
    """State container for the exploitability learning process.
    
    Attributes:
        key: JAX random key for reproducible randomness
        env_state: Current environment state
        last_timestep: Last timestep from environment
        agent: Learning agent (player being trained to exploit)
        target_agent: Target agent being evaluated for exploitability
        optimizer: Optimizer for the learning agent
        train_metrics: Training metrics accumulator
        rollout_metrics: Rollout metrics accumulator
    """
    key: chex.PRNGKey
    env_state: env_types.EnvState
    last_timestep: env_types.TimeStep
    agent: BaseAgent
    target_agent: BaseAgent
    optimizer: nnx.Optimizer
    train_metrics: nnx.MultiMetric
    rollout_metrics: nnx.MultiMetric


@partial(nnx.jit, static_argnames=('env', 'config'))
def single_training_step(
        learner_state: LearnerState,
        _: Any,
        env: env_types.BaseEnv,
        config: DictConfig
    ) -> Tuple[LearnerState, Any]:
    """Execute a single training step for exploitability computation.
    
    This function performs one iteration of data collection and policy update
    for training an agent to exploit the target agent.
    
    Args:
        learner_state: Current state of the learning process
        _: Unused scan variable (required by nnx.scan)
        env: Game environment
        config: Configuration containing algorithm hyperparameters
        
    Returns:
        Tuple of (updated_learner_state, None)
    """
    
    # Collect trajectories by having the learning agent play against the target agent 
    learner_state.key, collect_key = jax.random.split(learner_state.key)
    learner_state.env_state, learner_state.last_timestep, learner_state.rollout_metrics, dataset = collect_and_process_trajectories(
        env = env,
        agent = [learner_state.agent, learner_state.target_agent],
        env_state = learner_state.env_state,
        last_timestep = learner_state.last_timestep,
        metrics = learner_state.rollout_metrics,
        key = collect_key,
        num_envs = config.algorithm.num_envs,
        num_steps = config.algorithm.num_steps,
        gamma = config.algorithm.gamma,
        gae_gamma = config.algorithm.gae_gamma
    )

    # Update the learning agent using PPO to improve exploitation
    learner_state.key, update_key = jax.random.split(learner_state.key)
    learner_state.agent, learner_state.optimizer, learner_state.train_metrics = update_agent(
        agent = learner_state.agent,
        mag_agent = None,
        optimizer = learner_state.optimizer,
        dataset = dataset,
        metrics = learner_state.train_metrics,
        key = update_key,
        ent_coef = config.algorithm.ent_coef,
        mag_coef = 0,
        clip_eps = config.algorithm.clip_eps,
        num_minibatches = config.algorithm.num_minibatches,
        num_ppo_epoch = config.algorithm.num_ppo_epoch,
        only_use_player0_experience = True,
    )

    return learner_state, None


@partial(nnx.jit, static_argnames=('env', 'config'))
def training_step(
        learner_state: LearnerState,
        env: env_types.BaseEnv,
        config: DictConfig
    ) -> LearnerState:
    """Execute multiple training steps using scan for efficiency.
    
    Runs log_interval iterations of single_training_step to batch multiple
    updates together for computational efficiency.
    
    Args:
        learner_state: Current state of the learning process
        env: Game environment
        config: Configuration containing algorithm hyperparameters
        
    Returns:
        Updated learner state after log_interval training steps
    """
    learner_state, _ = nnx.scan(
        partial(single_training_step, env=env, config=config),
        length=config.logging.log_interval,
    )(learner_state, None)

    return learner_state


def get_avg_return(learner_state: LearnerState) -> float:
    """Extract and compute average return from rollout metrics.
    
    Processes the accumulated rollout metrics to compute the average return
    per episode, then resets the metrics for the next iteration.
    
    Args:
        learner_state: Current learner state containing accumulated metrics
        
    Returns:
        Average return per episode during the rollout phase
    """
    
    # Extract accumulated metrics
    rollout_metrics = learner_state.rollout_metrics.compute()
    

    # Compute average return from rollout metrics
    avg_return = rollout_metrics['reward'] / rollout_metrics['inverse_eps_len']
    
    # Reset metrics for next iteration
    learner_state.train_metrics.reset()
    learner_state.rollout_metrics.reset()

    return avg_return


def exploitability_rl_solver(target_agent: BaseAgent, _: env_types.BaseEnv, config: Any) -> float:
    """Compute exploitability by training an RL agent to exploit the target agent.
    
    This method trains a new agent using reinforcement learning to find the best
    response against the target agent. The final average return approximates the
    exploitability of the target agent.
    
    Args:
        target_agent: The agent whose exploitability is being measured
        env: Game environment for training the exploiting agent
        config: OmegaConf configuration containing training hyperparameters
        
    Returns:
        Approximated exploitability value (higher means more exploitable)
    """
    # Create a copy to avoid modifying the original config
    rng_key = jax.random.key(config.seed)

    # Initialize the training environment
    training_env = create_env(config.env.env_name)
    rng_key, env_init_key = jax.random.split(rng_key)
    env_init_keys = jax.random.split(env_init_key, config.algorithm.num_envs)
    initial_env_state, initial_timestep = jax.vmap(training_env.reset)(env_init_keys)

    # Create the exploiting agent (learning to exploit the target)
    rng_key, exploiting_agent_key = jax.random.split(rng_key)
    exploiting_agent = create_agent(config.agent.agent_name, key=exploiting_agent_key)

    # Initialize optimizer for the exploiting agent
    learning_rate = config.algorithm.lr
    optimizer = nnx.Optimizer(exploiting_agent, optax.adamw(learning_rate, eps=1e-5))
    
    # Set up training metrics tracking
    training_metrics = nnx.MultiMetric(
        actor_loss=nnx.metrics.Average("actor_loss"),
        ppo_loss=nnx.metrics.Average("ppo_loss"),
        entropy=nnx.metrics.Average("entropy"),
        critic_loss=nnx.metrics.Average("critic_loss"),
        approx_kl=nnx.metrics.Average("approx_kl"),
        mag_kl=nnx.metrics.Average("mag_kl"),  # Unused for exploitability computation
        clip_frac=nnx.metrics.Average("clip_frac"),
        explained_var=nnx.metrics.Average("explained_var"),
    )
    
    # Set up rollout metrics tracking
    rollout_metrics = nnx.MultiMetric(
        inverse_eps_len=nnx.metrics.Average("inverse_eps_len"),
        reward=nnx.metrics.Average("reward"),
    )

    # Initialize the learner state
    rng_key, learner_rng_key = jax.random.split(rng_key)
    learner_state = LearnerState(
        key=learner_rng_key,
        env_state=initial_env_state,
        last_timestep=initial_timestep,
        agent=exploiting_agent,
        target_agent=target_agent,
        optimizer=optimizer,
        train_metrics=training_metrics,
        rollout_metrics=rollout_metrics,
    )

    # Validate training configuration
    num_inner_updates = config.logging.save_interval
    log_interval = config.logging.log_interval
    assert num_inner_updates % log_interval == 0, "log_interval must be divisible by num_inner_update"
    
    # Main training loop for exploitability computation
    total_updates = num_inner_updates
    progress_desc = "Training exploiting agent"
    
    with tqdm(total=total_updates, desc=progress_desc) as progress_bar:
        for _ in range(0, total_updates, log_interval):
            # Perform training steps
            learner_state = training_step(learner_state, training_env, config)

            # Update progress tracking
            progress_bar.update(log_interval)

            # Log current performance
            current_return = get_avg_return(learner_state)
            progress_bar.set_description(f"{progress_desc} - return: {current_return:.4f}")

    return current_return  # Final return approximates exploitability


def load_log_data(log_file_path: str) -> Tuple[Dict, Any]:
    """Load and parse JSON log file.
    
    Args:
        log_file_path: Path to the JSON log file
        
    Returns:
        Tuple of (log_data_dict, parsed_config)
    """
    import json
    from omegaconf import OmegaConf
    
    with open(log_file_path, 'r') as f:
        log_data = json.load(f)
    
    config = OmegaConf.create(log_data['config'])
    return log_data, config


def compute_exploitability_rl(config: Any) -> List[Dict]:
    """Compute exploitability using RL solver for Nash PG agents.
    
    Args:
        config: Parsed configuration containing algorithm and logging parameters
        
    Returns:
        List of exploitability measurements with step information
    """
    from pathlib import Path
    
    eval_data = []
    num_inner_update = config.logging.save_interval
    checkpoint_dir = config.logging.checkpoint_dir
    
    # Create environment for exploitability computation
    env = create_env(config.env.env_name)
    
    # Find all available checkpoints
    checkpoint_base_dir = Path(checkpoint_dir) / Path(config.run_name)
    checkpoint_dirs = sorted([d for d in checkpoint_base_dir.glob("checkpoint_*") if d.is_dir()], key=lambda x: int(x.name.split('_')[1]))
    
    print(f"Found {len(checkpoint_dirs)} checkpoints to process")
    
    for checkpoint_dir_path in checkpoint_dirs:
        step = int(checkpoint_dir_path.name.split('_')[1])
        outer_loop_step = step // num_inner_update
        
        print(f"  Computing exploitability for step {step} (outer loop {outer_loop_step})")
        
        # Load target agent from checkpoint
        key = jax.random.key(config.seed)
        key, subkey = jax.random.split(key)
        target_agent = create_agent(config.agent.agent_name, subkey).load_checkpoint(
            checkpoint_dir=str(checkpoint_base_dir),
            step=step,
            key=key
        )
        
        # Compute exploitability using RL
        exploitability = exploitability_rl_solver(target_agent, env, config)
        
        eval_data.append({
            "step": int(step),
            "outer_loop_step": int(outer_loop_step), 
            "exploitability": float(exploitability)
        })
    
    return eval_data


def compute_exploitability_from_return(log_data: Dict, num_inner_update: int) -> List[Dict]:
    """Compute exploitability using FROM_RETURN solver for FSP/PSRO agents.
    
    Args:
        log_data: Complete log data dictionary
        num_inner_update: Number of inner loop updates per outer loop
        
    Returns:
        List of exploitability measurements with step information
    """
    eval_data = []
    rollout_data = log_data['rollout']
    
    # Process returns: get first entry and every entry where step % num_inner_update == 0
    for entry in rollout_data:
        step = entry['step']
        if step % num_inner_update == 0:
            outer_loop_step = step // num_inner_update - 1
            # For 2-player zero-sum games, exploitability â‰ˆ optimal opponent average_return
            exploitability = entry['return']
            
            eval_data.append({
                "step": int(outer_loop_step * num_inner_update),
                "outer_loop_step": int(outer_loop_step), 
                "exploitability": float(exploitability)
            })
    
    return eval_data


def save_updated_log(log_file_path: str, log_data: Dict, eval_data: List[Dict]) -> None:
    """Save updated log data with exploitability measurements.
    
    Args:
        log_file_path: Path to the JSON log file
        log_data: Original log data dictionary
        eval_data: Computed exploitability measurements
    """
    import json
    
    # Add exploitability data to existing eval entries or create new ones
    if 'eval' not in log_data:
        log_data['eval'] = []
    
    # Update existing eval entries or create new ones
    for eval_entry in eval_data:
        step = eval_entry['step']
        # Find existing entry with same step or create new one
        existing_entry = None
        for entry in log_data['eval']:
            if entry['step'] == step:
                existing_entry = entry
                break
        
        if existing_entry is None:
            # Create new entry
            log_data['eval'].append({
                'step': step,
                'outer_loop_step': eval_entry['outer_loop_step'],
                'exploitability': eval_entry['exploitability']
            })
        else:
            # Update existing entry
            existing_entry['outer_loop_step'] = eval_entry['outer_loop_step']
            existing_entry['exploitability'] = eval_entry['exploitability']
    
    # Write updated JSON back to file
    with open(log_file_path, 'w') as f:
        json.dump(log_data, f, indent=2)
    
    print(f"Successfully updated {log_file_path} with {len(eval_data)} exploitability entries")


def parse_arguments():
    """Parse command line arguments.
    
    Returns:
        argparse.Namespace: Parsed arguments containing file_path, solver_method, and cuda_device
    """
    parser = argparse.ArgumentParser(
        description="Compute exploitability metrics for trained RL agents",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""Examples:
  python compute_exploitability.py logs/kuhn_poker/nash_pg/mag0.1.json rl
  python compute_exploitability.py logs/kuhn_poker/fsp/run1.json from_return
        """
    )
    
    parser.add_argument(
        "file_path",
        type=str,
        help="Path to the JSON log file to process"
    )
    
    parser.add_argument(
        "solver_method", 
        type=str,
        choices=["rl", "from_return"],
        help="Exploitability computation method: 'rl' for RL solver (accurate but slow), 'from_return' for FROM_RETURN solver (fast, suitable for FSP/PSRO)"
    )
    
    
    return parser.parse_args()


def main():
    """Main function to process a single log file and compute exploitability."""
    # Parse command line arguments
    args = parse_arguments()
    
    # Convert string to enum
    solver_type = ExploitabilitySolver.RL if args.solver_method == "rl" else ExploitabilitySolver.FROM_RETURN
    
    print(f"Processing {args.file_path} with {solver_type.value} solver...")
    
    try:
        # Load log data and configuration
        log_data, config = load_log_data(args.file_path)
        num_inner_update = config.algorithm.num_inner_update
        
        # Compute exploitability based on solver type
        if solver_type == ExploitabilitySolver.RL:
            eval_data = compute_exploitability_rl(config)
        elif solver_type == ExploitabilitySolver.FROM_RETURN:
            eval_data = compute_exploitability_from_return(log_data, num_inner_update)
        else:
            raise ValueError(f"Unknown solver type: {solver_type}")
        
        # Save updated log with exploitability data
        save_updated_log(args.file_path, log_data, eval_data)
        print(f"Completed processing {args.file_path}")
        
    except FileNotFoundError:
        print(f"Error: File not found: {args.file_path}")
        return 1
    except Exception as e:
        print(f"Error processing {args.file_path}: {str(e)}")
        return 1
    
    return 0


if __name__ == "__main__":
    main()

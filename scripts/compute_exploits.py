#!/usr/bin/env python3
"""
Parallel Exploitability Computation Script for Multi-Algorithm Training

This script runs exploitability computation for multiple algorithm log files
(Nash PG, FSP, PSRO) in parallel across multiple GPUs with load balancing.
It processes files of the form:
- Nash PG: logs/{ENV}/nash_pg/mag{mag}_run{i}.json
- FSP: logs/{ENV}/fsp/default_run{i}.json
- PSRO: logs/{ENV}/psro/default_run{i}.json

Features:
- Distributes work across multiple GPUs for optimal resource utilization
- Limits concurrent processes per GPU (default: 3) to prevent overload
- Provides real-time progress tracking and error reporting
- Supports Nash PG, FSP, and PSRO algorithms
- Validates file existence before processing
- Graceful error handling with detailed status reporting

Usage:
  uv run scripts/compute_exploits.py --num-gpus 4
  uv run scripts/compute_exploits.py --num-gpus 4 --algorithms fsp psro
  uv run scripts/compute_exploits.py --num-gpus 4 --max-concurrent-per-gpu 3
"""

import asyncio
import argparse
import logging
from pathlib import Path
from typing import List, Tuple
from datetime import datetime
import time

# Configuration constants
MAG_COEFFICIENTS = [0.2]
MMD_COEFFICIENTS = [0.05]
DIVERGENCE_TYPES = ["kl"]
NUM_RUNS = 4
ALGORITHMS = ["nash_pg", "fsp", "psro", "mmd"]

# Default environment
DEFAULT_ENV = "kuhn_poker"

# Set up logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        # logging.FileHandler('scripts/npg_compute_exploits.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)


def generate_file_paths(algorithms: List[str], mag_coefs: List[float], mmd_coefs: List[float], divergence_types: List[str], num_runs: int, env: str) -> List[str]:
    """Generate all log file paths to process.
    
    Args:
        algorithms: List of algorithms to process (nash_pg, fsp, psro, mmd)
        mag_coefs: List of magnetic coefficient values (only used for nash_pg)
        mmd_coefs: List of MMD coefficient values (only used for mmd)
        divergence_types: List of divergence types (only used for nash_pg)
        num_runs: Number of runs per configuration
        env: Environment name
    
    Returns:
        List of file paths to process
    """
    # Define log file patterns based on environment
    log_file_patterns = {
        "nash_pg": f"logs/{env}/nash_pg/mag{{mag}}_{{divergence}}_run{{run}}.json",
        "mmd": f"logs/{env}/mmd/coef{{coef}}_run{{run}}.json",
        "fsp": f"logs/{env}/fsp/default_run{{run}}.json",
        "psro": f"logs/{env}/psro/default_run{{run}}.json"
    }
    
    file_paths = []
    for algorithm in algorithms:
        if algorithm == "nash_pg":
            # Nash PG variants use different magnetic coefficients and divergence types
            for mag in mag_coefs:
                for divergence in divergence_types:
                    for run in range(num_runs):
                        file_path = log_file_patterns[algorithm].format(mag=mag, divergence=divergence, run=run)
                        file_paths.append(file_path)
        elif algorithm == "mmd":
            # MMD variants use different MMD coefficients
            for coef in mmd_coefs:
                for run in range(num_runs):
                    file_path = log_file_patterns[algorithm].format(coef=coef, run=run)
                    file_paths.append(file_path)
        else:
            # FSP and PSRO don't use coefficients or divergence types
            for run in range(num_runs):
                file_path = log_file_patterns[algorithm].format(run=run)
                file_paths.append(file_path)
    
    return file_paths


def validate_files(file_paths: List[str]) -> Tuple[List[str], List[str]]:
    """Validate that log files exist.
    
    Args:
        file_paths: List of file paths to validate
        
    Returns:
        Tuple of (valid_files, missing_files)
    """
    valid_files = []
    missing_files = []
    
    for file_path in file_paths:
        if Path(file_path).exists():
            valid_files.append(file_path)
        else:
            missing_files.append(file_path)
    
    return valid_files, missing_files


class GPUTaskManager:
    """Manages parallel task execution across multiple GPUs with concurrency limits."""
    
    def __init__(self, num_gpus: int, max_concurrent_per_gpu: int = 3):
        """Initialize GPU task manager.
        
        Args:
            num_gpus: Number of GPUs to use
            max_concurrent_per_gpu: Maximum concurrent processes per GPU
        """
        self.num_gpus = num_gpus
        self.max_concurrent_per_gpu = max_concurrent_per_gpu
        self.gpu_semaphores = [asyncio.Semaphore(max_concurrent_per_gpu) for _ in range(num_gpus)]
        self.last_dispatch_time = [0.0] * num_gpus  # Track last dispatch time per GPU
        self.gpu_locks = [asyncio.Lock() for _ in range(num_gpus)]  # Locks for timing coordination
        self.completed_tasks = 0
        self.failed_tasks = 0
        self.total_tasks = 0
        self.start_time = None
        
    async def run_exploitability_computation(self, file_path: str, gpu_id: int) -> Tuple[str, bool, str]:
        """Run exploitability computation for a single file on specified GPU.
        
        Args:
            file_path: Path to the log file to process
            gpu_id: GPU device ID to use
            
        Returns:
            Tuple of (file_path, success, error_message)
        """
        # Wait for GPU dispatch timing (10 seconds between dispatches to same GPU)
        async with self.gpu_locks[gpu_id]:
            current_time = time.time()
            time_since_last_dispatch = current_time - self.last_dispatch_time[gpu_id]
            
            if time_since_last_dispatch < 60.0:
                wait_time = 60.0 - time_since_last_dispatch
                logger.info(f"GPU {gpu_id} waiting {wait_time:.1f}s before dispatch: {file_path}")
                await asyncio.sleep(wait_time)
            
            self.last_dispatch_time[gpu_id] = time.time()
        
        async with self.gpu_semaphores[gpu_id]:
            # Determine algorithm type from file path and set appropriate parameter
            if "/nash_pg/" in file_path or "/mmd/" in file_path:
                exploit_param = "rl"
            else:  # FSP or PSRO
                exploit_param = "from_return"
            
            cmd = f"CUDA_VISIBLE_DEVICES={gpu_id} uv run eval/compute_exploitability.py {file_path} {exploit_param}"
            
            try:
                logger.info(f"Starting GPU {gpu_id}: {file_path}")
                
                # Run the command
                process = await asyncio.create_subprocess_shell(
                    cmd,
                    stdout=asyncio.subprocess.PIPE,
                    stderr=asyncio.subprocess.PIPE
                )
                
                _, stderr = await process.communicate()
                
                if process.returncode == 0:
                    self.completed_tasks += 1
                    logger.info(f"✓ GPU {gpu_id} completed: {file_path}")
                    return file_path, True, ""
                else:
                    self.failed_tasks += 1
                    error_msg = stderr.decode() if stderr else f"Process failed with return code {process.returncode}"
                    logger.error(f"✗ GPU {gpu_id} failed: {file_path} - {error_msg}")
                    return file_path, False, error_msg
                    
            except Exception as e:
                self.failed_tasks += 1
                error_msg = str(e)
                logger.error(f"✗ GPU {gpu_id} exception: {file_path} - {error_msg}")
                return file_path, False, error_msg
    
    async def process_all_files(self, file_paths: List[str]) -> List[Tuple[str, bool, str]]:
        """Process all files in parallel across GPUs.
        
        Args:
            file_paths: List of file paths to process
            
        Returns:
            List of results (file_path, success, error_message)
        """
        self.total_tasks = len(file_paths)
        self.start_time = datetime.now()
        
        logger.info(f"Starting parallel processing of {self.total_tasks} files across {self.num_gpus} GPUs")
        logger.info(f"Max concurrent processes per GPU: {self.max_concurrent_per_gpu}")
        
        # Create tasks with GPU assignment (round-robin distribution)
        tasks = []
        for i, file_path in enumerate(file_paths):
            gpu_id = i % self.num_gpus
            task = self.run_exploitability_computation(file_path, gpu_id)
            tasks.append(task)
        
        # Start progress monitoring
        progress_task = asyncio.create_task(self._monitor_progress())
        
        # Execute all tasks
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # Cancel progress monitoring
        progress_task.cancel()
        
        # Process results and handle exceptions
        final_results = []
        for result in results:
            if isinstance(result, Exception):
                self.failed_tasks += 1
                final_results.append(("unknown", False, str(result)))
            else:
                final_results.append(result)
        
        self._print_final_summary()
        return final_results
    
    async def _monitor_progress(self):
        """Monitor and report progress every 30 seconds."""
        try:
            while True:
                await asyncio.sleep(60*10)
                completed = self.completed_tasks + self.failed_tasks
                if completed > 0:
                    elapsed = datetime.now() - self.start_time
                    rate = completed / elapsed.total_seconds() * 60  # files per minute
                    remaining = self.total_tasks - completed
                    eta_minutes = remaining / rate if rate > 0 else 0
                    
                    logger.info(f"Progress: {completed}/{self.total_tasks} "
                               f"({completed/self.total_tasks*100:.1f}%) - "
                               f"Rate: {rate:.1f} files/min - "
                               f"ETA: {eta_minutes:.1f} min")
        except asyncio.CancelledError:
            pass
    
    def _print_final_summary(self):
        """Print final execution summary."""
        elapsed = datetime.now() - self.start_time
        logger.info("="*60)
        logger.info("EXECUTION SUMMARY")
        logger.info("="*60)
        logger.info(f"Total files processed: {self.total_tasks}")
        logger.info(f"Successful: {self.completed_tasks}")
        logger.info(f"Failed: {self.failed_tasks}")
        logger.info(f"Success rate: {self.completed_tasks/self.total_tasks*100:.1f}%")
        logger.info(f"Total time: {elapsed}")
        logger.info(f"Average time per file: {elapsed.total_seconds()/self.total_tasks:.1f}s")


def parse_arguments():
    """Parse command line arguments.
    
    Returns:
        argparse.Namespace: Parsed arguments
    """
    parser = argparse.ArgumentParser(
        description="Run exploitability computation for Nash PG, FSP, and PSRO logs in parallel across GPUs",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""Examples:
  python scripts/compute_exploits.py --num-gpus 4 --env kuhn_poker
  python scripts/compute_exploits.py --num-gpus 2 --max-concurrent-per-gpu 2 --env liar_dice
  python scripts/compute_exploits.py --num-gpus 4 --algorithms fsp psro --env kuhn_poker
  python scripts/compute_exploits.py --num-gpus 1 --algorithms nash_pg --mag-coefs 0.0 0.1
  python scripts/compute_exploits.py --num-gpus 1 --max-concurrent-per-gpu 1 --dry-run
        """
    )
    
    parser.add_argument(
        "--num-gpus",
        type=int,
        default=4,
        help="Number of GPUs to use for parallel processing (default: 1)"
    )
    
    parser.add_argument(
        "--max-concurrent-per-gpu",
        type=int,
        default=1,
        help="Maximum concurrent processes per GPU (default: 3)"
    )
    
    parser.add_argument(
        "--dry-run",
        action="store_true",
        help="Show what would be processed without actually running commands"
    )
    
    parser.add_argument(
        "--algorithms",
        nargs="+",
        choices=ALGORITHMS,
        default=["nash_pg", "mmd", "fsp", "psro"],
        help=f"Algorithms to process (default: nash_pg, fsp, psro)"
    )
    
    parser.add_argument(
        "--mag-coefs",
        type=float,
        nargs="+",
        default=MAG_COEFFICIENTS,
        help=f"Magnetic coefficient values for Nash PG (default: {MAG_COEFFICIENTS})"
    )
    
    parser.add_argument(
        "--divergence-types",
        nargs="+",
        choices=DIVERGENCE_TYPES,
        default=DIVERGENCE_TYPES,
        help=f"Divergence types for Nash PG (default: {DIVERGENCE_TYPES})"
    )
    
    parser.add_argument(
        "--mmd-coefs",
        type=float,
        nargs="+",
        default=MMD_COEFFICIENTS,
        help=f"MMD coefficient values for MMD (default: {MMD_COEFFICIENTS})"
    )
    
    parser.add_argument(
        "--num-runs",
        type=int,
        default=NUM_RUNS,
        help=f"Number of runs per configuration (default: {NUM_RUNS})"
    )
    
    parser.add_argument(
        "--verbose",
        action="store_true",
        help="Enable verbose output"
    )
    
    parser.add_argument(
        "--env",
        type=str,
        default=DEFAULT_ENV,
        help=f"Environment name (default: {DEFAULT_ENV})"
    )
    
    return parser.parse_args()


async def main():
    """Main function."""
    args = parse_arguments()
    
    if args.verbose:
        logging.getLogger().setLevel(logging.DEBUG)
    
    # Generate all file paths
    logger.info("Generating file paths...")
    file_paths = generate_file_paths(args.algorithms, args.mag_coefs, args.mmd_coefs, args.divergence_types, args.num_runs, args.env)
    logger.info(f"Generated {len(file_paths)} file paths to process")
    logger.info(f"Environment: {args.env}")
    logger.info(f"Algorithms: {args.algorithms}")
    logger.info(f"Magnetic coefficients (for Nash PG): {args.mag_coefs}")
    logger.info(f"MMD coefficients (for MMD): {args.mmd_coefs}")
    logger.info(f"Divergence types (for Nash PG): {args.divergence_types}")
    logger.info(f"Runs per configuration: {args.num_runs}")
    
    # Validate files exist
    logger.info("Validating file existence...")
    valid_files, missing_files = validate_files(file_paths)
    
    if missing_files:
        logger.warning(f"Missing {len(missing_files)} files:")
        for missing_file in missing_files:
            logger.warning(f"  - {missing_file}")
    
    if not valid_files:
        logger.error("No valid files found to process!")
        return 1
    
    logger.info(f"Found {len(valid_files)} valid files to process")
    
    if args.dry_run:
        logger.info("DRY RUN MODE - Would process the following files:")
        for i, file_path in enumerate(valid_files):
            gpu_id = i % args.num_gpus
            logger.info(f"  GPU {gpu_id}: {file_path}")
        logger.info(f"Total: {len(valid_files)} files across {args.num_gpus} GPUs")
        return 0
    
    # Process files
    task_manager = GPUTaskManager(args.num_gpus, args.max_concurrent_per_gpu)
    results = await task_manager.process_all_files(valid_files)
    
    # Report failed files
    failed_files = [result for result in results if not result[1]]
    if failed_files:
        logger.error(f"\nFailed to process {len(failed_files)} files:")
        for file_path, _, error in failed_files:
            logger.error(f"  - {file_path}: {error}")
    
    return 0 if task_manager.failed_tasks == 0 else 1


if __name__ == "__main__":
    try:
        exit_code = asyncio.run(main())
        exit(exit_code)
    except KeyboardInterrupt:
        logger.info("\nInterrupted by user")
        exit(1)
    except Exception as e:
        logger.error(f"Unexpected error: {e}")
        exit(1)